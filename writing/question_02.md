# Question 2

_Sound in interactive media traditionally plays a complementary role, augmenting, complementing, or clarifying visual and tactile interactions. Discuss: 1) how breaking this synchronicity—resulting in what Schafer calls schizophonia—can pave the way for sound autonomy, 2) how this autonomy can come to define sound-centric interactions and narratives, and 3) what challenges this poses to information transmission within the action-reaction loop.

## INTRO

Interaction Design is one of the more broad design disciplines, with a great number of elements involved. We can focus on a given design's use of space, time, texture, appearance, etc, but one of the core (and yet often overlooked) aspects is sound. Even the most vision-forward or seemingly tactile-focused design includes a sonic element. The keyboard that I am currently using to type these words is a perfect example. The color, shape, and font usage on the keys have clearly been thought out, but there has been serious attention paid to the way it sounds as well. The clicking sound as I type is not too loud as to disturb my neighbors in a coffee shop, but is giving me aural feedback to let me know that I have successfully entered the correct character. 

footnote: Steve Jobs is said to have forced his engineers to dissassemble and reassemble every single iPod the night before it was unveiled with a new headphone jack because the one they had chosen didn't make a satisfying click when the headphones were inserted. https://www.npr.org/sections/alltechconsidered/2010/12/30/132477388/undesigned-the-symbiotic-relationship-of-steve-jobs-and-jonathan-ive

This attention to sound is important in interaction design and user-experience design, and much has been written about how to make it the most successful. Traditional music composition, sonic broadcast media, narrative screen-based media, and interactive games have influenced this conversation. Looking at these various fields helps designers learn a great deal about how sound can support the goals of a given interaction and emotional goal, while also pointing the way for new opportunities once we subvert and break this support-centered correlation. 

### SUPPORT SOUND

Film was the first of the twentieth century media to recognize the importance of sound, with the credit of "sound designer" introduced in the 1970s. Sound designers craft the sonic elements of their films to support visuals through traditional "foley" techniques mimicing on-screen events (the soft crunch of walking through falling snow, the brittle snap of a breaking bone) or by using digital recording techniques and processes to augment real-world sounds and create entirely manufactured ones. As they work, these designers often think about sounds as being either diegetic (happening in the world represented by the visuals) or nondiegetic (those sounds outside of the world such as voiceovers and soundtrack). 

Both the diegetic and nondiegetic sounds can be manipulated and processed to strengthen the themes that are presented on screen. Sound designer David Sonnenschein explains how sound can "functions as an interpretive element, guiding the listener towards a certain feeling, subjectively beyond the visual elements." While this is most common with nondiegetic music (the darkness and militaristic pomp of "The Imperial March" as Darth Vader is first introduced in *Star Wars*), sound designers can go to great lengths to create diegetic sound effects that also help in that interpretative element (the combination of bear, badger, and dog sounds for Chewbacca's voice that help cement the character as a powerful, dangerous, and stalwart companion). Sound designers for interactive media apply lessons from these crafted nondiegetic sounds, creating "earcons" that communicate ideas about certain interactions (the *whoosh* of sending mail, or the reprimanding *thonk* of an undesired click).

The world of games introduces player agency as an element of sound design. Karen Collins separates these interactive sounds into nondynamic (which hew more traditionally to the diegetic/nondiegetic divide) and dynamic (which are triggered by game state, game environment, or player interaction). These dynamic sounds are particularly useful in both communicating important information to the player and as an opportunity to reinforce the game world and emotional content. For instance, in her analysis of the sound design of *Super Mario Galaxy*'s level-select menu, Elizabeth Medina-Gray comments that

"instead of a bare-bones and entirely functional static sound, the variety of musical pitches elicited by this single and otherwise mundane action might elevate the action to a more playful or joyful mode of engagement, and encourage a player to further experiment with the system (both the actions and the sounds)"

This playfulness is strengthened by the sonic qualities of that interaction. In fact, as designer Brenda Laurel claims, the "\[t]ight linkage between visual, kinesthetic, and auditory modalities is the key to the sense of immersion that is created by many computer games, simulations, and virtual-reality systems."

However, many practitioners and scholars lament the backseat nature of sound in the design of an overall experience. Rick Altman discusses what he refers to as "the ontological fallacy," where film critics have argued that "image without sound still constitutes cinema, while sound without an image is no longer cinema." Altman claims that these arguments are purposefully made to negate the influence of sound on film. Similarly, sound designer Rob Bridgett complains of sound-focused (and often even sound-led) game production teams, "falling back on the old scheduling methodologies that prevent sound from being in a true leadership role." Even Dan Saffer's *Designing for Interaction* includes sound as one of the key elements yet states that it "is underutilized (some would say rightfully so) in interaction design."  

## ACOUSMATIC SOUND

What happens, however, when sound does not play a secondary role to the interactive and/or visual goals, and instead functions in a separate and equally important context?

In *The New Soundscape*, R. Murray Schafer introduces the idea of "schizophonia," which, when revisiting the topic in *The Tuning of the World (The Soundscape)*, he floridly explains as the phenomena of sounds

_torn from their natural sockets and given amplified and independent existence. Vocal sound, for instance, is no longer tied to a hole in the head but it is free to issue from anywhere in the landscape. In the same instant it may issue from millions of holes in millions of public and private places around the world _

Schafer is skeptical of how recording and transmission technology changes our relationship to the sound around us. Jonathan Sterne, in his introduction to the section on recorded media in *The Sound Studies Reader*, likens Schafer's distrust to Plato's *The Phaedrus* where Socrates prefers living speech over dead paintings because "they go on telling you just one thing forever." We often see this mistrust with new technologies (fear of losing the written word with the advent of the ebook or creativity with the rise of AI tools), but looking back on earlier thought on the topic is perhaps more instructive.

footnote: Schafer explains in The New Soundscape that he has deliberately named schizophonia in relation to schizophrenia to indicate "the same sense of aberration and drama."

In his *Treatise on Musical Objects,* french composer and electrical engineer Pierre Schaeffer introduces the concept of "acousmatics." For Schaeffer, the fundamental divorce of sound from sound-making object that recorded media facilitates creates an entirely new listening experience. His understanding builds on Edmund Husserl's concept of phenomenology, claiming that the new "sonorous object" is more than the original by encompassing the acoustic signal, playback method, playback location, reverberation, and listener perception. Additionally (and for Schaeffer, excitingly), the acousmatic no longer relies on the visual for context. In acousmatic listening, "we listen to the sonorous forms, without any aim other than of hearing them better, in order to be able to describe them through an analysis of our perceptions."

Schaeffer's own *musique concrete* is an example of acousmatics in practice. By isolating the sound of a train from the experience of standing in a train-yard in _Étude aux chemins de fer_, Schaeffer presents the listener with an entirely new sensation. Further, by manipulating these sounds through chopping, looping, and electronically processing, *musique concrete* brings to the acoustic space sounds that had never existed before. Where Schafer senses danger, Schaeffer sees potential and possibility. What can sound designers do then with this new potentiality of sound divorced from its visual counterparts?

Footnote - Jason Stanyek and Benjamin Piekut propose the term "rhizophonia" as a replacement arguing that schizophonia seeks to describe an exception (sound as separate from source) when in reality "all sound are severed from their sources--that's what makes sound sound."

## SOUND POSSIBILITIES

When we begin to think of sound apart from a supporting role, new possibilities open up to us.

### Sound Autonomy

One potentiality that presents itself for sound in interactive media is brand new combinations of sound and visuals. On her discussion of Schafer's schizophonia in *Playing with Sound*, Karen Collins comments that "more remarkable than the separation of sound from source, however, is our ability to reassociate or integrate that schizophonic sound with a new visual source to create new meanings." Aaron Oldenburg echos this claim, stating "acousmatic sounds play a particularly large role in mainstream horror games, sparking the imagination and fear of unseen presences, or by disturbing the player with a sense of the uncanny through sounds that do not match their assumptions about the environment."

This mirrors how sound designers for film create and strengthen emotional and narrative content through their recombinational methods, what film sound theorist Michel Chion's refers to as "synchresis." Relying on physical, psychological, and cultural foundations of certain sounds, designers imbue their soundscapes with new meaning. As mentioned above, these techniques can be pushed to extremes where wholly unrelated sound/visual combinations present radical experiences. A baby's scream replaced with a train whistle or tea kettle communicates new information to the viewer about what this particular episode might signify, a phenomena which sound designer Walter Murch refers to as "conceptual resonance."

### Sound-centric interactions

There is also the possibility for completely sound-focused interactions, where *visuals* fill the support role or are otherwise completely absent. Both games and non-playful-interactions have explored this idea, and the genre of audio-only games has risen from this design goal. While there is a long history of audio-focused games (the electronic game *Simon* from 1978 is an early example), games like *Papa Sangre*, *Dark Echo*, and *Blind Drive* create entirely new worlds and experiences for the player, using only sonic elements. These games are reminiscent of radio plays in the early 20th century, which enable listeners to create and visualize entire worlds. For many, these worlds are more robust and real than a screen-based version could ever be. Hugill et al explain how players in *Papa Sangre* "have to rely on imagination to supply the visual component, allowing for some amusing creative possibilities within the game, such as the sound of squeaky toys that are heard when you step on 'bones' that might alert the snuffle hog."

With the ascent of voice-controlled home-control systems such as Google Nest and Amazon's Echo there is even more opportunity for sound-forward interaction design. Designers can create entire ecosystems of control and feedback, using sound as the reference point. Since these sonic interactions (as with games) are not dependent on physical properties of sounding bodies and spatial acoustics, designers are free to create new soundscapes with no real reference to physical space. Blesser et al. explain that "space no longer serves the role of providing seating for the audience. Space becomes the aural equivalent of Escher’s bizarre pictures of impossible objects and spatial geometries: artistic illusions."

Footnote: We see this in other popular media, as in Marilyn Robinson's Pulizter Prize-winning novel Gilead, where the narrator says ""

### Multimodal Interactions

Further, games in particular allow for an even deeper relationship as they require physical action on the part of the player. Collins refers to these as multimodal interactions, and claims that "the stakes for players' involvement, interpretation, and therefore attention are much higher in games, so they listen more actively and employ different modes of listening to guide their own movements and actions in the game. Although film may act *on* the body, players act *with* games."

Beyond this multimodal and referential method of synchresis, acousmatic sound design has the ability to imbue sounds with even deeper meaning. Sonification, which seeks to sonically  communicate complex and often large sets of data (temperature, brain wave patterns, etc.) using sound, is of particular interest here. As Axel Berndt states in his paper from the Ambient@40 conference in 2018:

_It is now possible to use any data stream to modulate one or more parameters automatically. This could be, for instance, the distance between player position and level exit, the number of non-player characters in the scene, the complexity of a puzzle, health meter, mouse cursor movement, or sensor data (e.g. light, accelerometer, microphone). Music now functions as a sonification of this input data, a Musical Auditory Display. [...] Such a Musical Auditory Display establishes a much closer relation to player interaction and the game’s diegesis. It can help to create an even more personalized player experience._

These acousmatic and data-imbued sounds have the potential to cover more than one of Chion's listening modes (casual, semantic, reduced), making it possible for even deeper interpretations of the soundscape. As Friberg and Gärdenfors claim, "by targeting more than one listening mode, sounds can carry several layers of information, resulting in a more multi-faceted and appealing game audio content."

### Challenges and Future Directions

As we build these new sound-centric interaction, a few challenges arise. 

First, the very interactive nature of these experiences blurs the line between listener and performer, sometimes making it difficult to ascertain the "true meaning" of a given sound. As William O'Hara notes in his analysis of *Proteus*, "the collapse of the performer/listener dichotomy is traumatic: it means the dissolution of the Cartesian subject/object divide that underlies much of Western musical thought." 

Second, it is well documented that there are limits to the sonic fidelity that most users can interperet. This is not to say that there isn't potential for growth in this area, but that the current state of user feedback relies heavily on the visual, and possibly because of this, is more widely understood by the public. Because of this, there is the potential for some listeners to miss pertinent information. 

Within these challenges, however, are where the most difficult and exciting possibilities exist. Blurring the composer/listener boundary could possibly create more ambiguity and room for artistic expression. Similarly, multimodal sounds could have layered meaning within Chion's listening modes, with the most important sounds being easily heard and understood on the casual level, with more nuanced and deeper ideas available for those who choose to deeply listen.




